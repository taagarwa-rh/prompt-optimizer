{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Prompt Optimizer","text":"<p>Improve your prompts with any LLM using Automatic Prompt Optimization (APO).</p>"},{"location":"#overview","title":"Overview","text":"From \"A Systematic Survey of Automatic Prompt Optimization Techniques\" <p>Automatic prompt optimization (APO) is a reinforcement learning technique to improve prompt performance. At each iteration, new prompts are generated and scored against your AI system using your validation set. Promising prompts are kept and used to seed the next generation of prompts. The goal is to find the prompt that maximizes the AI system's performance on the evaluation metric you define.</p>"},{"location":"#installation","title":"Installation","text":"<p>uv (recommended)</p> <pre><code>uv add git+https://github.com/taagarwa-rh/prompt-optimizer.git\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#getting-started","title":"Getting Started","text":"<p>Before you can use an optimizer, you must define your validation set and evaluator.</p> <p>The validation set is a set of examples for your task, and should be a list of dictionaries. For example, a simple QA validation set might look like:</p> <pre><code>validation_set = [\n    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n    {\"question\": \"What is the largest planet in our solar system?\", \"answer\": \"Jupiter\"},\n    {\"question\": \"What is the smallest planet in our solar system?\", \"answer\": \"Mercury\"},\n]\n</code></pre> <p>The evaluator is your scoring function for generated prompts. It should take a prompt and your validation set and produce a set of predictions, one for each example in the validation set. That score should represent how much you value the predictions from a prompt. For example, a simple QA evaluator might look like:</p> <pre><code>from datetime import datetime\n\nfrom langchain_openai import ChatOpenAI\nfrom prompt_optimizer import BasePrompt\n\n\ndef evaluator(prompt: BasePrompt, validation_set: list[dict]) -&gt; float:\n    \"\"\"Prompt evaluator function.\"\"\"\n    predictions = []\n    num_correct = 0\n    llm = ChatOpenAI(model=\"gpt-5\")\n    for row in validation_set:\n        # Get the prediction from your system\n        question = row[\"question\"]\n        messages = [{\"role\": \"system\", \"content\": prompt.content}, {\"role\": \"user\", \"content\": question}]\n        response = llm.invoke(messages)\n        prediction = response.content.strip()\n        predictions.append(prediction)\n\n        # Reward exact matches\n        actual = row[\"answer\"]\n        if actual == prediction:\n            num_correct += 1\n        else:\n            num_correct += 0\n\n    # Compute the score\n    score = num_correct / len(validation_set)\n\n    # Optionally, save the predictions and other info in metadata\n    prompt.metadata[\"predictions\"] = predictions\n    prompt.metadata[\"run_date\"] = datetime.now()\n\n    return score\n</code></pre>"},{"location":"#using-optimizers","title":"Using Optimizers","text":"<p>Once you have your validation set and evaluator defined, you can set up an optimization pipeline.</p> <p>Select one of the available optimizers to learn more about its usage:</p> <ul> <li>OPRO (Recommended)</li> <li>ProTeGi</li> <li>APE</li> </ul>"},{"location":"#citations","title":"Citations","text":"<pre><code>@inproceedings{Ramnath_2025,\n   title={A Systematic Survey of Automatic Prompt Optimization Techniques},\n   url={http://dx.doi.org/10.18653/v1/2025.emnlp-main.1681},\n   DOI={10.18653/v1/2025.emnlp-main.1681},\n   booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},\n   publisher={Association for Computational Linguistics},\n   author={Ramnath, Kiran and Zhou, Kang and Guan, Sheng and Mishra, Soumya Smruti and Qi, Xuan and Shen, Zhengyuan and Wang, Shuai and Woo, Sangmin and Jeoung, Sullam and Wang, Yawei and Wang, Haozhu and Ding, Han and Lu, Yuzhe and Xu, Zhichao and Zhou, Yun and Srinivasan, Balasubramaniam and Yan, Qiaojing and Chen, Yueyan and Ding, Haibo and Xu, Panpan and Cheong, Lin Lee},\n   year={2025},\n   pages={33066\u201333098} }\n</code></pre>"},{"location":"library/","title":"Overview","text":""},{"location":"library/optimizers/","title":"Overview","text":""},{"location":"library/optimizers/ape/","title":"Automatic Prompt Engineer (APE)","text":""},{"location":"library/optimizers/ape/#about","title":"About","text":"<p>Automatic Prompt Engineer (APE) starts by generating a set of prompt candidates through \"forward generation\". During forward generation, input and output pairs from the validation set are presented to the language model, and the model is tasked with generating the instructions that could be used to answer that question. After these initial prompts are scored, the top k% scoring prompts are retained. On the following iterations, these prompts are resampled by asking a language model to create a variation of an existing prompt, then scoring and selecting the top k% scoring prompts. This process continues until the maximum iteration depth is reached or the score threshold is exceeded.</p>"},{"location":"library/optimizers/ape/#citation","title":"Citation","text":"<pre><code>@misc{zhou2023largelanguagemodelshumanlevel,\n    title={Large Language Models Are Human-Level Prompt Engineers}, \n    author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},\n    year={2023},\n    eprint={2211.01910},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG},\n    url={https://arxiv.org/abs/2211.01910}, \n}\n</code></pre>"},{"location":"library/optimizers/ape/#source","title":"Source","text":""},{"location":"library/optimizers/ape/#src.prompt_optimizer.optimizers.ape.APEOptimizer","title":"<code>APEOptimizer</code>","text":"<p>               Bases: <code>BaseOptimizer</code></p> <p>APE Optimizer.</p> <p>Based on Automatic Prompt Engineer from Zhou, et. al.</p> <pre><code>@misc{zhou2023largelanguagemodelshumanlevel,\n    title={Large Language Models Are Human-Level Prompt Engineers},\n    author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},\n    year={2023},\n    eprint={2211.01910},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG},\n    url={https://arxiv.org/abs/2211.01910},\n}\n</code></pre> Source code in <code>src/prompt_optimizer/optimizers/ape.py</code> <pre><code>class APEOptimizer(BaseOptimizer):\n    \"\"\"\n    APE Optimizer.\n\n    Based on Automatic Prompt Engineer from Zhou, et. al.\n\n    ```\n    @misc{zhou2023largelanguagemodelshumanlevel,\n        title={Large Language Models Are Human-Level Prompt Engineers},\n        author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},\n        year={2023},\n        eprint={2211.01910},\n        archivePrefix={arXiv},\n        primaryClass={cs.LG},\n        url={https://arxiv.org/abs/2211.01910},\n    }\n    ```\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        client: ClientType,\n        validation_set: ValidationSetType,\n        max_depth: int,\n        evaluator: Callable[[BasePrompt, ValidationSetType], ScoreType],\n        output_path: Optional[Union[str, Path]] = None,\n        input_field: str,\n        output_field: str,\n        num_initial_prompts: int = 10,\n        num_exemplars: int = 5,\n        k_percent: float = 0.5,\n        score_threshold: Optional[Union[float, int]] = None,\n    ):\n        \"\"\"\n        Initialize the APE optimizer.\n\n        Args:\n            client (ClientType):\n                Language model client to use for prompt generation and feedback.\n            validation_set (ValidationSetType):\n                Set of examples to evaluate the prompt on.\n            max_depth (int):\n                Maximum iteration depth for prompt generation.\n            evaluator (Callable[[BasePrompt, ValidationSetType], ScoreType]):\n                Function that takes a prompt and the validation data and returns a score.\n            output_path (Union[str, Path], optional):\n                Path to store run results. Should be a .jsonl file path.\n                If None, no outputs will be written to disk. Defaults to None.\n            input_field (str):\n                Field in the validation set that represents the input. Used in forward generation in\n                the \"Input:\" field.\n            output_field (str):\n                Field in the validation set that represents the output. Used in forward generation in\n                the \"Output:\" field.\n            num_initial_prompts (int):\n                Number of prompts to create in the initial forward generation. Defaults to 10.\n            num_exemplars (int):\n                Number of exemplars from the validation set to provide for forward generation.\n                A random sample of input and output pairs of this size will be provided to the LLM\n                during forward generation. Defaults to 5.\n            k_percent (str, optional):\n                Top k% of candidate prompts to retain between iterations. Defaults to 0.5.\n            score_threshold (float, optional):\n                Threshold for early convergence. If a prompt exceeds this score after any iteration, the optimization loop\n                immediately ends. If set to None, the optimization loop will not terminate early. Defaults to None.\n        \"\"\"\n        super().__init__(\n            client=client,\n            seed_prompts=[],  # There is no seeding for APE\n            validation_set=validation_set,\n            max_depth=max_depth,\n            evaluator=evaluator,\n            output_path=output_path,\n        )\n        self.num_initial_prompts = num_initial_prompts\n        self.num_exemplars = num_exemplars\n        self.input_field = input_field\n        self.output_field = output_field\n        self.k_percent = k_percent\n        self.score_threshold = score_threshold\n\n    def _generate(self, metaprompt_template: str, template_kwargs: dict) -&gt; str:\n        \"\"\"\n        Generate a completion for a given template and kwargs and parse the results.\n\n        Args:\n            metaprompt_template (str): Template for the metaprompt.\n            template_kwargs (dict): Key word arguments to fill the template values.\n            kwargs: Additional kwargs to pass to the OpenAI client.completions.create (e.g. temperature)\n\n        Returns:\n            list[str]: The parsed generation results.\n\n        \"\"\"\n        metaprompt = metaprompt_template.format(**template_kwargs)\n        input = [{\"role\": \"user\", \"content\": metaprompt}]\n        raw_response = self.client.invoke(input=input)\n        response = raw_response.content.strip()\n        return response\n\n    def generate_prompt_candidates(self, *, prompts: list[BasePrompt], validation_set: ValidationSetType) -&gt; list[BasePrompt]:\n        \"\"\"Generate prompt candidates using forward generation or resampling.\"\"\"\n        prompt_candidates = []\n\n        # Do forward generation if there are no prompts\n        if len(prompts) == 0:\n            for i in track(range(self.num_initial_prompts), description=\"Generating new prompts\", transient=True):\n                sample = random.choices(validation_set, k=self.num_exemplars)\n                input_output_pairs = \"\\n\".join(\n                    [f\"**Input:** {exemplar[self.input_field]}   **Output:** {exemplar[self.output_field]}\" for exemplar in sample]\n                )\n                template_kwargs = {\"input_output_pairs\": input_output_pairs}\n                new_prompt = self._generate(metaprompt_template=FORWARD_GENERATION_TEMPLATE, template_kwargs=template_kwargs)\n                prompt_candidates.append(BasePrompt(content=new_prompt))\n\n        # Otherwise, resample the prompts\n        else:\n            for prompt in track(prompts, description=\"Generating new prompts\", transient=True):\n                template_kwargs = {\"instruction\": prompt.content}\n                new_prompt = self._generate(metaprompt_template=RESAMPLING_PROMPT_TEMPLATE, template_kwargs=template_kwargs)\n                prompt_candidates.append(BasePrompt(content=new_prompt))\n\n        return prompt_candidates\n\n    def _get_best_prompt(self, prompts: list[BasePrompt]):\n        if any(prompt.score is None for prompt in prompts):\n            raise ValueError(\"All prompts must be scored before calling this function.\")\n        return max(prompts, key=lambda x: x.score)\n\n    def select_prompt_candidates(self, *, prompts: list[BasePrompt], validation_set: ValidationSetType) -&gt; list[BasePrompt]:\n        \"\"\"Select the top scoring k% of prompts.\"\"\"\n        self._score_prompts(prompts=prompts, validation_set=validation_set)\n        # Select the top scoring k% of prompts\n        k = int(len(prompts) * self.k_percent)\n        best_prompts = sorted(prompts, key=lambda x: x.score, reverse=True)[:k]\n        return best_prompts\n\n    def check_early_convergence(self, *, all_prompts: list[list[BasePrompt]]):\n        \"\"\"Check if any prompt exceeds the score threshold.\"\"\"\n        if self.score_threshold is None:\n            return False\n\n        # Flatten all iterations\n        prompts = sum(all_prompts, start=[])\n\n        # Check if early convergence criteria is met\n        highest_score = max(prompts, key=lambda x: x.score).score\n        if highest_score &gt;= self.score_threshold:\n            return True\n        return False\n\n    def select_best_prompt(self, *, all_prompts: list[list[BasePrompt]]) -&gt; BasePrompt:\n        \"\"\"Select the highest scoring prompt.\"\"\"\n        # Flatten all iterations\n        prompts = sum(all_prompts, start=[])\n\n        # Select the single prompt with the highest score\n        best_prompt = self._get_best_prompt(prompts=prompts)\n        logger.info(f\"Best score: {best_prompt.score:.3f}\")\n        return best_prompt\n</code></pre>"},{"location":"library/optimizers/ape/#src.prompt_optimizer.optimizers.ape.APEOptimizer.__init__","title":"<code>__init__(*, client, validation_set, max_depth, evaluator, output_path=None, input_field, output_field, num_initial_prompts=10, num_exemplars=5, k_percent=0.5, score_threshold=None)</code>","text":"<p>Initialize the APE optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>ClientType</code> <p>Language model client to use for prompt generation and feedback.</p> required <code>validation_set</code> <code>ValidationSetType</code> <p>Set of examples to evaluate the prompt on.</p> required <code>max_depth</code> <code>int</code> <p>Maximum iteration depth for prompt generation.</p> required <code>evaluator</code> <code>Callable[[BasePrompt, ValidationSetType], ScoreType]</code> <p>Function that takes a prompt and the validation data and returns a score.</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to store run results. Should be a .jsonl file path. If None, no outputs will be written to disk. Defaults to None.</p> <code>None</code> <code>input_field</code> <code>str</code> <p>Field in the validation set that represents the input. Used in forward generation in the \"Input:\" field.</p> required <code>output_field</code> <code>str</code> <p>Field in the validation set that represents the output. Used in forward generation in the \"Output:\" field.</p> required <code>num_initial_prompts</code> <code>int</code> <p>Number of prompts to create in the initial forward generation. Defaults to 10.</p> <code>10</code> <code>num_exemplars</code> <code>int</code> <p>Number of exemplars from the validation set to provide for forward generation. A random sample of input and output pairs of this size will be provided to the LLM during forward generation. Defaults to 5.</p> <code>5</code> <code>k_percent</code> <code>str</code> <p>Top k% of candidate prompts to retain between iterations. Defaults to 0.5.</p> <code>0.5</code> <code>score_threshold</code> <code>float</code> <p>Threshold for early convergence. If a prompt exceeds this score after any iteration, the optimization loop immediately ends. If set to None, the optimization loop will not terminate early. Defaults to None.</p> <code>None</code> Source code in <code>src/prompt_optimizer/optimizers/ape.py</code> <pre><code>def __init__(\n    self,\n    *,\n    client: ClientType,\n    validation_set: ValidationSetType,\n    max_depth: int,\n    evaluator: Callable[[BasePrompt, ValidationSetType], ScoreType],\n    output_path: Optional[Union[str, Path]] = None,\n    input_field: str,\n    output_field: str,\n    num_initial_prompts: int = 10,\n    num_exemplars: int = 5,\n    k_percent: float = 0.5,\n    score_threshold: Optional[Union[float, int]] = None,\n):\n    \"\"\"\n    Initialize the APE optimizer.\n\n    Args:\n        client (ClientType):\n            Language model client to use for prompt generation and feedback.\n        validation_set (ValidationSetType):\n            Set of examples to evaluate the prompt on.\n        max_depth (int):\n            Maximum iteration depth for prompt generation.\n        evaluator (Callable[[BasePrompt, ValidationSetType], ScoreType]):\n            Function that takes a prompt and the validation data and returns a score.\n        output_path (Union[str, Path], optional):\n            Path to store run results. Should be a .jsonl file path.\n            If None, no outputs will be written to disk. Defaults to None.\n        input_field (str):\n            Field in the validation set that represents the input. Used in forward generation in\n            the \"Input:\" field.\n        output_field (str):\n            Field in the validation set that represents the output. Used in forward generation in\n            the \"Output:\" field.\n        num_initial_prompts (int):\n            Number of prompts to create in the initial forward generation. Defaults to 10.\n        num_exemplars (int):\n            Number of exemplars from the validation set to provide for forward generation.\n            A random sample of input and output pairs of this size will be provided to the LLM\n            during forward generation. Defaults to 5.\n        k_percent (str, optional):\n            Top k% of candidate prompts to retain between iterations. Defaults to 0.5.\n        score_threshold (float, optional):\n            Threshold for early convergence. If a prompt exceeds this score after any iteration, the optimization loop\n            immediately ends. If set to None, the optimization loop will not terminate early. Defaults to None.\n    \"\"\"\n    super().__init__(\n        client=client,\n        seed_prompts=[],  # There is no seeding for APE\n        validation_set=validation_set,\n        max_depth=max_depth,\n        evaluator=evaluator,\n        output_path=output_path,\n    )\n    self.num_initial_prompts = num_initial_prompts\n    self.num_exemplars = num_exemplars\n    self.input_field = input_field\n    self.output_field = output_field\n    self.k_percent = k_percent\n    self.score_threshold = score_threshold\n</code></pre>"},{"location":"library/optimizers/ape/#src.prompt_optimizer.optimizers.ape.APEOptimizer.check_early_convergence","title":"<code>check_early_convergence(*, all_prompts)</code>","text":"<p>Check if any prompt exceeds the score threshold.</p> Source code in <code>src/prompt_optimizer/optimizers/ape.py</code> <pre><code>def check_early_convergence(self, *, all_prompts: list[list[BasePrompt]]):\n    \"\"\"Check if any prompt exceeds the score threshold.\"\"\"\n    if self.score_threshold is None:\n        return False\n\n    # Flatten all iterations\n    prompts = sum(all_prompts, start=[])\n\n    # Check if early convergence criteria is met\n    highest_score = max(prompts, key=lambda x: x.score).score\n    if highest_score &gt;= self.score_threshold:\n        return True\n    return False\n</code></pre>"},{"location":"library/optimizers/ape/#src.prompt_optimizer.optimizers.ape.APEOptimizer.generate_prompt_candidates","title":"<code>generate_prompt_candidates(*, prompts, validation_set)</code>","text":"<p>Generate prompt candidates using forward generation or resampling.</p> Source code in <code>src/prompt_optimizer/optimizers/ape.py</code> <pre><code>def generate_prompt_candidates(self, *, prompts: list[BasePrompt], validation_set: ValidationSetType) -&gt; list[BasePrompt]:\n    \"\"\"Generate prompt candidates using forward generation or resampling.\"\"\"\n    prompt_candidates = []\n\n    # Do forward generation if there are no prompts\n    if len(prompts) == 0:\n        for i in track(range(self.num_initial_prompts), description=\"Generating new prompts\", transient=True):\n            sample = random.choices(validation_set, k=self.num_exemplars)\n            input_output_pairs = \"\\n\".join(\n                [f\"**Input:** {exemplar[self.input_field]}   **Output:** {exemplar[self.output_field]}\" for exemplar in sample]\n            )\n            template_kwargs = {\"input_output_pairs\": input_output_pairs}\n            new_prompt = self._generate(metaprompt_template=FORWARD_GENERATION_TEMPLATE, template_kwargs=template_kwargs)\n            prompt_candidates.append(BasePrompt(content=new_prompt))\n\n    # Otherwise, resample the prompts\n    else:\n        for prompt in track(prompts, description=\"Generating new prompts\", transient=True):\n            template_kwargs = {\"instruction\": prompt.content}\n            new_prompt = self._generate(metaprompt_template=RESAMPLING_PROMPT_TEMPLATE, template_kwargs=template_kwargs)\n            prompt_candidates.append(BasePrompt(content=new_prompt))\n\n    return prompt_candidates\n</code></pre>"},{"location":"library/optimizers/ape/#src.prompt_optimizer.optimizers.ape.APEOptimizer.get_all_prompts","title":"<code>get_all_prompts(include_candidates=False)</code>","text":"<p>Get all the prompts from the latest training run.</p> <p>The default behavior returns a list of lists, where each internal list contains the retained candidates after one iteration step. Setting include_candidates to True will also include all generated candidate prompts.</p> <p>Parameters:</p> Name Type Description Default <code>include_candidates</code> <code>bool</code> <p>Whether to include all the candidate prompts in the output. If True, candidate prompts from each iteration will be included. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[list[BasePrompt]]</code> <p>list[list[BasePrompt]]: List of lists where each list contains the prompts from each iteration. E.g. list[0] contains prompts from the first iteration, list[1] the second, etc. If include_candidates is False, each inner list contains only the retained prompts at each iteration. If include_candidates is True, each inner list contains all candidate prompts at each iteration, including those that were discarded.</p> Source code in <code>src/prompt_optimizer/optimizers/base.py</code> <pre><code>def get_all_prompts(self, include_candidates: bool = False) -&gt; list[list[BasePrompt]]:\n    \"\"\"\n    Get all the prompts from the latest training run.\n\n    The default behavior returns a list of lists, where each internal list contains the\n    retained candidates after one iteration step.\n    Setting include_candidates to True will also include all generated candidate prompts.\n\n    Args:\n        include_candidates (bool, optional):\n            Whether to include all the candidate prompts in the output.\n            If True, candidate prompts from each iteration will be included.\n            Defaults to False.\n\n    Returns:\n        list[list[BasePrompt]]:\n            List of lists where each list contains the prompts from each iteration.\n            E.g. list[0] contains prompts from the first iteration, list[1] the second, etc.\n            If include_candidates is False, each inner list contains only the retained prompts at each iteration.\n            If include_candidates is True, each inner list contains all candidate prompts at each iteration,\n            including those that were discarded.\n\n    \"\"\"\n    # Decide whether to include candidates\n    if include_candidates:\n        all_prompts = self._g\n    else:\n        all_prompts = self._p\n\n    return all_prompts\n</code></pre>"},{"location":"library/optimizers/ape/#src.prompt_optimizer.optimizers.ape.APEOptimizer.run","title":"<code>run()</code>","text":"<p>Run the optimization pipeline.</p> Source code in <code>src/prompt_optimizer/optimizers/base.py</code> <pre><code>def run(self) -&gt; BasePrompt:\n    \"\"\"Run the optimization pipeline.\"\"\"\n    # Initialize objects\n    self._p = [self.seed_prompts]\n    self._g = [self.seed_prompts]\n    # Iterate until max depth\n    for t in track(range(1, self.max_depth + 1), description=\"Step\", total=self.max_depth):\n        # Generate prompt candidates\n        g_t = self.generate_prompt_candidates(prompts=self._p[t - 1], validation_set=self.validation_set)\n        self._g.append(g_t)\n        # Select prompt candidates\n        p_t = self.select_prompt_candidates(prompts=self._g[t], validation_set=self.validation_set)\n        self._p.append(p_t)\n        # Check for early convergence\n        if self.check_early_convergence(all_prompts=self._p):\n            break\n\n    # Save prompts if requested\n    self.save_prompts(output_path=self.output_path)\n\n    # Return best prompt\n    return self.select_best_prompt(all_prompts=self._p)\n</code></pre>"},{"location":"library/optimizers/ape/#src.prompt_optimizer.optimizers.ape.APEOptimizer.save_prompts","title":"<code>save_prompts(output_path)</code>","text":"<p>Save prompts in jsonl format.</p> Source code in <code>src/prompt_optimizer/optimizers/base.py</code> <pre><code>def save_prompts(self, output_path: Optional[Union[str, Path]]):\n    \"\"\"Save prompts in jsonl format.\"\"\"\n    # Exit if no output path is set\n    if self.output_path is None:\n        return\n\n    # Get and deduplicate prompts\n    prompts = sum(self._p, start=[])\n    prompts = list(set(prompts))\n\n    # Save the prompts to the file\n    lines = [prompt.model_dump_json() for prompt in prompts]\n    with open(output_path, \"w\") as f:\n        for line in lines:\n            f.write(line)\n            f.write(\"\\n\")\n</code></pre>"},{"location":"library/optimizers/ape/#src.prompt_optimizer.optimizers.ape.APEOptimizer.select_best_prompt","title":"<code>select_best_prompt(*, all_prompts)</code>","text":"<p>Select the highest scoring prompt.</p> Source code in <code>src/prompt_optimizer/optimizers/ape.py</code> <pre><code>def select_best_prompt(self, *, all_prompts: list[list[BasePrompt]]) -&gt; BasePrompt:\n    \"\"\"Select the highest scoring prompt.\"\"\"\n    # Flatten all iterations\n    prompts = sum(all_prompts, start=[])\n\n    # Select the single prompt with the highest score\n    best_prompt = self._get_best_prompt(prompts=prompts)\n    logger.info(f\"Best score: {best_prompt.score:.3f}\")\n    return best_prompt\n</code></pre>"},{"location":"library/optimizers/ape/#src.prompt_optimizer.optimizers.ape.APEOptimizer.select_prompt_candidates","title":"<code>select_prompt_candidates(*, prompts, validation_set)</code>","text":"<p>Select the top scoring k% of prompts.</p> Source code in <code>src/prompt_optimizer/optimizers/ape.py</code> <pre><code>def select_prompt_candidates(self, *, prompts: list[BasePrompt], validation_set: ValidationSetType) -&gt; list[BasePrompt]:\n    \"\"\"Select the top scoring k% of prompts.\"\"\"\n    self._score_prompts(prompts=prompts, validation_set=validation_set)\n    # Select the top scoring k% of prompts\n    k = int(len(prompts) * self.k_percent)\n    best_prompts = sorted(prompts, key=lambda x: x.score, reverse=True)[:k]\n    return best_prompts\n</code></pre>"},{"location":"library/optimizers/opro/","title":"Optimization by PROmpting (OPRO)","text":""},{"location":"library/optimizers/opro/#about","title":"About","text":"<p>Optimization by PROmpting (OPRO) starts with a seed prompt. At each iteration, a collection of past prompt candidates and scores and a random sample of input and output pairs from the validation set are formatted into a metaprompt. The metaprompt is sent to the language model multiple times, each time asking it to provide a new candidate prompt that improves the instructions for the task. These new candidates are scored and the iterations continue until the maximum depth is reached or the score threshold is exceeded.</p>"},{"location":"library/optimizers/opro/#citation","title":"Citation","text":"<pre><code>@misc{yang2024largelanguagemodelsoptimizers,\n    title={Large Language Models as Optimizers},\n    author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},\n    year={2024},\n    eprint={2309.03409},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG},\n    url={https://arxiv.org/abs/2309.03409},\n}\n</code></pre>"},{"location":"library/optimizers/opro/#source","title":"Source","text":""},{"location":"library/optimizers/opro/#src.prompt_optimizer.optimizers.opro.OPROOptimizer","title":"<code>OPROOptimizer</code>","text":"<p>               Bases: <code>BaseOptimizer</code></p> <p>OPRO Optimizer.</p> <p>Based on Optimization by PROmpting from Yang, et. al. 2024</p> <pre><code>@misc{yang2024largelanguagemodelsoptimizers,\n    title={Large Language Models as Optimizers},\n    author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},\n    year={2024},\n    eprint={2309.03409},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG},\n    url={https://arxiv.org/abs/2309.03409},\n}\n</code></pre> Source code in <code>src/prompt_optimizer/optimizers/opro.py</code> <pre><code>class OPROOptimizer(BaseOptimizer):\n    \"\"\"\n    OPRO Optimizer.\n\n    Based on Optimization by PROmpting from Yang, et. al. 2024\n\n    ```\n    @misc{yang2024largelanguagemodelsoptimizers,\n        title={Large Language Models as Optimizers},\n        author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},\n        year={2024},\n        eprint={2309.03409},\n        archivePrefix={arXiv},\n        primaryClass={cs.LG},\n        url={https://arxiv.org/abs/2309.03409},\n    }\n    ```\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        client: ClientType,\n        seed_prompts: list[BasePrompt],\n        validation_set: ValidationSetType,\n        max_depth: int,\n        evaluator: Callable[[BasePrompt, ValidationSetType], ScoreType],\n        output_path: Optional[Union[str, Path]] = None,\n        input_field: str,\n        output_field: str,\n        num_candidates_per_step: int = 20,\n        num_exemplars: int = 3,\n        max_demonstration_prompts: int = 20,\n        score_threshold: Optional[Union[float, int]] = None,\n    ):\n        \"\"\"\n        Initialize the APE optimizer.\n\n        Args:\n            client (ClientType):\n                Language model client to use for prompt generation and feedback.\n            seed_prompts (list[BasePrompt]):\n                List of prompts to seed generation.\n            validation_set (ValidationSetType):\n                Set of examples to evaluate the prompt on.\n            max_depth (int):\n                Maximum iteration depth for prompt generation.\n            evaluator (Callable[[BasePrompt, ValidationSetType], ScoreType]):\n                Function that takes a prompt and the validation data and returns a score.\n            output_path (Union[str, Path], optional):\n                Path to store run results. Should be a .jsonl file path.\n                If None, no outputs will be written to disk. Defaults to None.\n            input_field (str):\n                Field in the validation set that represents the input. Used in candidate generation in\n                the \"input:\" field.\n            output_field (str):\n                Field in the validation set that represents the output. Used in candidate generation in\n                the \"output:\" field.\n            num_candidates_per_step (int, optional):\n                Number of candidates to create at each step. Defaults to 20.\n            num_exemplars (int, optional):\n                Number of exemplars from the validation set to provide in the metaprompt.\n                A random sample of input and output pairs of this size will be provided to the LLM\n                during candidate generation. Defaults to 3.\n            max_demonstration_prompts (int, optional):\n                Maximum number of demostration prompts to provide in the metaprompt.\n                Defaults to 20.\n            score_threshold (float, optional):\n                Threshold for early convergence. If a prompt exceeds this score after any iteration, the optimization loop\n                immediately ends. If set to None, the optimization loop will not terminate early. Defaults to None.\n        \"\"\"\n        super().__init__(\n            client=client,\n            seed_prompts=seed_prompts,\n            validation_set=validation_set,\n            max_depth=max_depth,\n            evaluator=evaluator,\n            output_path=output_path,\n        )\n        self.num_candidates_per_step = num_candidates_per_step\n        self.num_exemplars = num_exemplars\n        self.max_demonstration_prompts = max_demonstration_prompts\n        self.input_field = input_field\n        self.output_field = output_field\n        self.score_threshold = score_threshold\n\n    def _extract_response(self, content: str) -&gt; str:\n        \"\"\"\n        Extract the responses between square brackets.\n\n        Args:\n            content (str): Output string from an LLM generation request.\n\n        Returns:\n            str: Response captured between square brackets, or the whole response if there are no square brackets.\n\n        \"\"\"\n        if \"[\" not in content or \"]\" not in content:\n            return content\n\n        # Get everything after the first bracket\n        parsed_content = content.split(\"[\", maxsplit=1)[1]\n\n        # Get everything before the last bracket\n        parsed_content = parsed_content[::-1].split(\"]\", maxsplit=1)[1][::-1]\n\n        return parsed_content\n\n    def _generate(self, metaprompt_template: str, template_kwargs: dict) -&gt; str:\n        \"\"\"\n        Generate a completion for a given template and kwargs and parse the results.\n\n        Args:\n            metaprompt_template (str): Template for the metaprompt.\n            template_kwargs (dict): Key word arguments to fill the template values.\n            kwargs: Additional kwargs to pass to the OpenAI client.completions.create (e.g. temperature)\n\n        Returns:\n            list[str]: The parsed generation results.\n\n        \"\"\"\n        metaprompt = metaprompt_template.format(**template_kwargs)\n        input = [{\"role\": \"user\", \"content\": metaprompt}]\n        raw_response = self.client.invoke(input=input)\n        response = raw_response.content.strip()\n        return response\n\n    def generate_prompt_candidates(self, *, prompts: list[BasePrompt], validation_set: ValidationSetType) -&gt; list[BasePrompt]:\n        \"\"\"Generate prompt candidates.\"\"\"\n        # Sort prompts by score, highest to lowest\n        sorted_prompts = sorted(prompts, key=lambda x: x.score, reverse=True)[: self.max_demonstration_prompts]\n\n        # Format prompts into instructions_and_scores\n        instructions_and_scores = \"\\n\\n\".join([f\"text:\\n{prompt.content}\\nscore:\\n{prompt.score}\" for prompt in sorted_prompts])\n\n        # Generate prompt candidates\n        prompt_candidates = []\n        for _ in track(range(self.num_candidates_per_step), description=\"Generating prompt candidates\", transient=True):\n            # Format a sample of questions into input_output_pairs\n            sample = random.choices(validation_set, k=self.num_exemplars)\n            input_output_pairs = \"\\n\\n\".join(\n                [f\"input:\\n&lt;INS&gt;\\n{row[self.input_field]}\\noutput:\\n{row[self.output_field]}\" for row in sample]\n            )\n\n            # Generate prompt candidate\n            template_kwargs = {\"instructions_and_scores\": instructions_and_scores, \"input_output_pairs\": input_output_pairs}\n            response = self._generate(metaprompt_template=METAPROMPT_TEMPLATE, template_kwargs=template_kwargs)\n            prompt_candidate = self._extract_response(response)\n\n            prompt_candidates.append(BasePrompt(content=prompt_candidate))\n\n        # Return the original prompts plus the new candidates\n        return prompts + prompt_candidates\n\n    def select_prompt_candidates(self, *, prompts: list[BasePrompt], validation_set: ValidationSetType) -&gt; list[BasePrompt]:\n        \"\"\"Select prompt candidates.\"\"\"\n        # Score the prompts\n        self._score_prompts(prompts=prompts, validation_set=validation_set)\n        # Return all prompts, no filtering is done\n        return prompts\n\n    def check_early_convergence(self, *, all_prompts: list[list[BasePrompt]]) -&gt; bool:\n        \"\"\"Detect early convergence.\"\"\"\n        if self.score_threshold is None:\n            return False\n\n        # Flatten all iterations\n        prompts = sum(all_prompts, start=[])\n\n        # Check if early convergence criteria is met\n        highest_score = max(prompts, key=lambda x: x.score).score\n        if highest_score &gt;= self.score_threshold:\n            return True\n        return False\n\n    def _get_best_prompt(self, prompts: list[BasePrompt]):\n        \"\"\"Get the highest scoring prompt.\"\"\"\n        if any(prompt.score is None for prompt in prompts):\n            raise ValueError(\"All prompts must be scored before calling this function.\")\n        return max(prompts, key=lambda x: x.score)\n\n    def select_best_prompt(self, all_prompts: list[list[BasePrompt]]) -&gt; BasePrompt:\n        \"\"\"Select the top scoring prompt.\"\"\"\n        # Flatten all iterations\n        prompts = sum(all_prompts, start=[])\n\n        # Select the single prompt with the highest score\n        best_prompt = self._get_best_prompt(prompts=prompts)\n        logger.info(f\"Best score: {best_prompt.score:.3f}\")\n        return best_prompt\n</code></pre>"},{"location":"library/optimizers/opro/#src.prompt_optimizer.optimizers.opro.OPROOptimizer.__init__","title":"<code>__init__(*, client, seed_prompts, validation_set, max_depth, evaluator, output_path=None, input_field, output_field, num_candidates_per_step=20, num_exemplars=3, max_demonstration_prompts=20, score_threshold=None)</code>","text":"<p>Initialize the APE optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>ClientType</code> <p>Language model client to use for prompt generation and feedback.</p> required <code>seed_prompts</code> <code>list[BasePrompt]</code> <p>List of prompts to seed generation.</p> required <code>validation_set</code> <code>ValidationSetType</code> <p>Set of examples to evaluate the prompt on.</p> required <code>max_depth</code> <code>int</code> <p>Maximum iteration depth for prompt generation.</p> required <code>evaluator</code> <code>Callable[[BasePrompt, ValidationSetType], ScoreType]</code> <p>Function that takes a prompt and the validation data and returns a score.</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to store run results. Should be a .jsonl file path. If None, no outputs will be written to disk. Defaults to None.</p> <code>None</code> <code>input_field</code> <code>str</code> <p>Field in the validation set that represents the input. Used in candidate generation in the \"input:\" field.</p> required <code>output_field</code> <code>str</code> <p>Field in the validation set that represents the output. Used in candidate generation in the \"output:\" field.</p> required <code>num_candidates_per_step</code> <code>int</code> <p>Number of candidates to create at each step. Defaults to 20.</p> <code>20</code> <code>num_exemplars</code> <code>int</code> <p>Number of exemplars from the validation set to provide in the metaprompt. A random sample of input and output pairs of this size will be provided to the LLM during candidate generation. Defaults to 3.</p> <code>3</code> <code>max_demonstration_prompts</code> <code>int</code> <p>Maximum number of demostration prompts to provide in the metaprompt. Defaults to 20.</p> <code>20</code> <code>score_threshold</code> <code>float</code> <p>Threshold for early convergence. If a prompt exceeds this score after any iteration, the optimization loop immediately ends. If set to None, the optimization loop will not terminate early. Defaults to None.</p> <code>None</code> Source code in <code>src/prompt_optimizer/optimizers/opro.py</code> <pre><code>def __init__(\n    self,\n    *,\n    client: ClientType,\n    seed_prompts: list[BasePrompt],\n    validation_set: ValidationSetType,\n    max_depth: int,\n    evaluator: Callable[[BasePrompt, ValidationSetType], ScoreType],\n    output_path: Optional[Union[str, Path]] = None,\n    input_field: str,\n    output_field: str,\n    num_candidates_per_step: int = 20,\n    num_exemplars: int = 3,\n    max_demonstration_prompts: int = 20,\n    score_threshold: Optional[Union[float, int]] = None,\n):\n    \"\"\"\n    Initialize the APE optimizer.\n\n    Args:\n        client (ClientType):\n            Language model client to use for prompt generation and feedback.\n        seed_prompts (list[BasePrompt]):\n            List of prompts to seed generation.\n        validation_set (ValidationSetType):\n            Set of examples to evaluate the prompt on.\n        max_depth (int):\n            Maximum iteration depth for prompt generation.\n        evaluator (Callable[[BasePrompt, ValidationSetType], ScoreType]):\n            Function that takes a prompt and the validation data and returns a score.\n        output_path (Union[str, Path], optional):\n            Path to store run results. Should be a .jsonl file path.\n            If None, no outputs will be written to disk. Defaults to None.\n        input_field (str):\n            Field in the validation set that represents the input. Used in candidate generation in\n            the \"input:\" field.\n        output_field (str):\n            Field in the validation set that represents the output. Used in candidate generation in\n            the \"output:\" field.\n        num_candidates_per_step (int, optional):\n            Number of candidates to create at each step. Defaults to 20.\n        num_exemplars (int, optional):\n            Number of exemplars from the validation set to provide in the metaprompt.\n            A random sample of input and output pairs of this size will be provided to the LLM\n            during candidate generation. Defaults to 3.\n        max_demonstration_prompts (int, optional):\n            Maximum number of demostration prompts to provide in the metaprompt.\n            Defaults to 20.\n        score_threshold (float, optional):\n            Threshold for early convergence. If a prompt exceeds this score after any iteration, the optimization loop\n            immediately ends. If set to None, the optimization loop will not terminate early. Defaults to None.\n    \"\"\"\n    super().__init__(\n        client=client,\n        seed_prompts=seed_prompts,\n        validation_set=validation_set,\n        max_depth=max_depth,\n        evaluator=evaluator,\n        output_path=output_path,\n    )\n    self.num_candidates_per_step = num_candidates_per_step\n    self.num_exemplars = num_exemplars\n    self.max_demonstration_prompts = max_demonstration_prompts\n    self.input_field = input_field\n    self.output_field = output_field\n    self.score_threshold = score_threshold\n</code></pre>"},{"location":"library/optimizers/opro/#src.prompt_optimizer.optimizers.opro.OPROOptimizer.check_early_convergence","title":"<code>check_early_convergence(*, all_prompts)</code>","text":"<p>Detect early convergence.</p> Source code in <code>src/prompt_optimizer/optimizers/opro.py</code> <pre><code>def check_early_convergence(self, *, all_prompts: list[list[BasePrompt]]) -&gt; bool:\n    \"\"\"Detect early convergence.\"\"\"\n    if self.score_threshold is None:\n        return False\n\n    # Flatten all iterations\n    prompts = sum(all_prompts, start=[])\n\n    # Check if early convergence criteria is met\n    highest_score = max(prompts, key=lambda x: x.score).score\n    if highest_score &gt;= self.score_threshold:\n        return True\n    return False\n</code></pre>"},{"location":"library/optimizers/opro/#src.prompt_optimizer.optimizers.opro.OPROOptimizer.generate_prompt_candidates","title":"<code>generate_prompt_candidates(*, prompts, validation_set)</code>","text":"<p>Generate prompt candidates.</p> Source code in <code>src/prompt_optimizer/optimizers/opro.py</code> <pre><code>def generate_prompt_candidates(self, *, prompts: list[BasePrompt], validation_set: ValidationSetType) -&gt; list[BasePrompt]:\n    \"\"\"Generate prompt candidates.\"\"\"\n    # Sort prompts by score, highest to lowest\n    sorted_prompts = sorted(prompts, key=lambda x: x.score, reverse=True)[: self.max_demonstration_prompts]\n\n    # Format prompts into instructions_and_scores\n    instructions_and_scores = \"\\n\\n\".join([f\"text:\\n{prompt.content}\\nscore:\\n{prompt.score}\" for prompt in sorted_prompts])\n\n    # Generate prompt candidates\n    prompt_candidates = []\n    for _ in track(range(self.num_candidates_per_step), description=\"Generating prompt candidates\", transient=True):\n        # Format a sample of questions into input_output_pairs\n        sample = random.choices(validation_set, k=self.num_exemplars)\n        input_output_pairs = \"\\n\\n\".join(\n            [f\"input:\\n&lt;INS&gt;\\n{row[self.input_field]}\\noutput:\\n{row[self.output_field]}\" for row in sample]\n        )\n\n        # Generate prompt candidate\n        template_kwargs = {\"instructions_and_scores\": instructions_and_scores, \"input_output_pairs\": input_output_pairs}\n        response = self._generate(metaprompt_template=METAPROMPT_TEMPLATE, template_kwargs=template_kwargs)\n        prompt_candidate = self._extract_response(response)\n\n        prompt_candidates.append(BasePrompt(content=prompt_candidate))\n\n    # Return the original prompts plus the new candidates\n    return prompts + prompt_candidates\n</code></pre>"},{"location":"library/optimizers/opro/#src.prompt_optimizer.optimizers.opro.OPROOptimizer.get_all_prompts","title":"<code>get_all_prompts(include_candidates=False)</code>","text":"<p>Get all the prompts from the latest training run.</p> <p>The default behavior returns a list of lists, where each internal list contains the retained candidates after one iteration step. Setting include_candidates to True will also include all generated candidate prompts.</p> <p>Parameters:</p> Name Type Description Default <code>include_candidates</code> <code>bool</code> <p>Whether to include all the candidate prompts in the output. If True, candidate prompts from each iteration will be included. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[list[BasePrompt]]</code> <p>list[list[BasePrompt]]: List of lists where each list contains the prompts from each iteration. E.g. list[0] contains prompts from the first iteration, list[1] the second, etc. If include_candidates is False, each inner list contains only the retained prompts at each iteration. If include_candidates is True, each inner list contains all candidate prompts at each iteration, including those that were discarded.</p> Source code in <code>src/prompt_optimizer/optimizers/base.py</code> <pre><code>def get_all_prompts(self, include_candidates: bool = False) -&gt; list[list[BasePrompt]]:\n    \"\"\"\n    Get all the prompts from the latest training run.\n\n    The default behavior returns a list of lists, where each internal list contains the\n    retained candidates after one iteration step.\n    Setting include_candidates to True will also include all generated candidate prompts.\n\n    Args:\n        include_candidates (bool, optional):\n            Whether to include all the candidate prompts in the output.\n            If True, candidate prompts from each iteration will be included.\n            Defaults to False.\n\n    Returns:\n        list[list[BasePrompt]]:\n            List of lists where each list contains the prompts from each iteration.\n            E.g. list[0] contains prompts from the first iteration, list[1] the second, etc.\n            If include_candidates is False, each inner list contains only the retained prompts at each iteration.\n            If include_candidates is True, each inner list contains all candidate prompts at each iteration,\n            including those that were discarded.\n\n    \"\"\"\n    # Decide whether to include candidates\n    if include_candidates:\n        all_prompts = self._g\n    else:\n        all_prompts = self._p\n\n    return all_prompts\n</code></pre>"},{"location":"library/optimizers/opro/#src.prompt_optimizer.optimizers.opro.OPROOptimizer.run","title":"<code>run()</code>","text":"<p>Run the optimization pipeline.</p> Source code in <code>src/prompt_optimizer/optimizers/base.py</code> <pre><code>def run(self) -&gt; BasePrompt:\n    \"\"\"Run the optimization pipeline.\"\"\"\n    # Initialize objects\n    self._p = [self.seed_prompts]\n    self._g = [self.seed_prompts]\n    # Iterate until max depth\n    for t in track(range(1, self.max_depth + 1), description=\"Step\", total=self.max_depth):\n        # Generate prompt candidates\n        g_t = self.generate_prompt_candidates(prompts=self._p[t - 1], validation_set=self.validation_set)\n        self._g.append(g_t)\n        # Select prompt candidates\n        p_t = self.select_prompt_candidates(prompts=self._g[t], validation_set=self.validation_set)\n        self._p.append(p_t)\n        # Check for early convergence\n        if self.check_early_convergence(all_prompts=self._p):\n            break\n\n    # Save prompts if requested\n    self.save_prompts(output_path=self.output_path)\n\n    # Return best prompt\n    return self.select_best_prompt(all_prompts=self._p)\n</code></pre>"},{"location":"library/optimizers/opro/#src.prompt_optimizer.optimizers.opro.OPROOptimizer.save_prompts","title":"<code>save_prompts(output_path)</code>","text":"<p>Save prompts in jsonl format.</p> Source code in <code>src/prompt_optimizer/optimizers/base.py</code> <pre><code>def save_prompts(self, output_path: Optional[Union[str, Path]]):\n    \"\"\"Save prompts in jsonl format.\"\"\"\n    # Exit if no output path is set\n    if self.output_path is None:\n        return\n\n    # Get and deduplicate prompts\n    prompts = sum(self._p, start=[])\n    prompts = list(set(prompts))\n\n    # Save the prompts to the file\n    lines = [prompt.model_dump_json() for prompt in prompts]\n    with open(output_path, \"w\") as f:\n        for line in lines:\n            f.write(line)\n            f.write(\"\\n\")\n</code></pre>"},{"location":"library/optimizers/opro/#src.prompt_optimizer.optimizers.opro.OPROOptimizer.select_best_prompt","title":"<code>select_best_prompt(all_prompts)</code>","text":"<p>Select the top scoring prompt.</p> Source code in <code>src/prompt_optimizer/optimizers/opro.py</code> <pre><code>def select_best_prompt(self, all_prompts: list[list[BasePrompt]]) -&gt; BasePrompt:\n    \"\"\"Select the top scoring prompt.\"\"\"\n    # Flatten all iterations\n    prompts = sum(all_prompts, start=[])\n\n    # Select the single prompt with the highest score\n    best_prompt = self._get_best_prompt(prompts=prompts)\n    logger.info(f\"Best score: {best_prompt.score:.3f}\")\n    return best_prompt\n</code></pre>"},{"location":"library/optimizers/opro/#src.prompt_optimizer.optimizers.opro.OPROOptimizer.select_prompt_candidates","title":"<code>select_prompt_candidates(*, prompts, validation_set)</code>","text":"<p>Select prompt candidates.</p> Source code in <code>src/prompt_optimizer/optimizers/opro.py</code> <pre><code>def select_prompt_candidates(self, *, prompts: list[BasePrompt], validation_set: ValidationSetType) -&gt; list[BasePrompt]:\n    \"\"\"Select prompt candidates.\"\"\"\n    # Score the prompts\n    self._score_prompts(prompts=prompts, validation_set=validation_set)\n    # Return all prompts, no filtering is done\n    return prompts\n</code></pre>"},{"location":"library/optimizers/protegi/","title":"Prompt Optimization with Textual Gradients (ProTeGi)","text":""},{"location":"library/optimizers/protegi/#about","title":"About","text":"<p>Prompt Optimization with Textual Gradients (ProTeGi) starts from an initial prompt and iteratively updates using feedback from the language model, called \"gradients\". The initial prompt is scored using the validation set and the failures are collected. The failures are described to the language model, and the language model is asked to provide multiple feedbacks (gradients) to describe how the prompt should be improved. The language model then uses each of these gradients to generate a number of new prompts, and these new prompts are scored. Then the language model generates variations The top scoring prompt is retained and becomes the initial prompt for the next iteration.  The iterations continue until the maximum iteration depth is reached or the score threshold is exceeded.</p> <p>A variation of this using \"greedy\" search instead keeps all prompts at each iteration, leading to larger trees of prompts.</p>"},{"location":"library/optimizers/protegi/#usage","title":"Usage","text":"<p>The <code>ProtegiOptimizer</code> requires a description of the failures after each step. You must provide this feedback in your evaluator by assigning an <code>\"error_string\"</code> in the prompt's metadata.</p> <p>[!IMPORTANT] Important Note Your evaluator function **MUST** create the \"error_string\" and save it in metadata. Otherwise the optimization will fail.</p> <pre><code>from lagnchain_openai import ChatOpenAI\nfrom prompt_optimizer import ProtegiOptimizer\n\n# Simple QA validation set\nvalidation_set = [\n    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n    {\"question\": \"What is the largest planet in our solar system?\", \"answer\": \"Jupiter\"},\n    {\"question\": \"What is the smallest planet in our solar system?\", \"answer\": \"Mercury\"},\n    {\"question\": \"What is the longest river in the world?\", \"answer\": \"Nile\"},\n    {\"question\": \"What is the smallest river in the world?\", \"answer\": \"Reprua River\"},\n]\n\n# A langchain ChatModel for generating new prompts\nclient = ChatOpenAI(model=\"gpt-5\", temperature=0.7)\n\n# Evaluator function\ndef evaluator(prompt: BasePrompt, validation_set: list[dict]) -&gt; list[str]:\n    \"\"\"Prompt evaluator function.\"\"\"\n    # Run the prompt through the AI system\n    predictions = []\n    errors = []\n    num_correct = 0\n    agent = get_agent()\n    for row in validation_set:\n        question = row[\"input\"]\n        messages = [{\"role\": \"system\", \"content\": prompt.content}, {\"role\": \"user\", \"content\": question}]\n        response = agent.invoke(messages)\n        prediction = response.content.strip()\n        predictions.append(prediction)\n\n        # Reward exact matches and collect errors\n        actual = row[\"target\"]\n        if actual == prediction:\n            num_correct += 1\n        else:\n            num_correct += 0\n            error = f\"Question: '{question}'\\nPrediction: '{prediction}'\\nActual Answer: {actual}\"\n            errors.append(error)\n\n    # Compute the score\n    score = num_correct / len(validation_set)\n\n    # Save the error string (required for ProtegiOptimizer)\n    prompt.metadata[\"error_string\"] = \"\\n\\n\".join(errors)\n\n    return score\n\n# Initialize the optimizer\noptimizer = ProtegiOptimizer(\n    client=client,\n    seed_prompts=[baseline_prompt],\n    validation_set=validation_set,\n    evaluator=evaluator,\n)\n\n# Run the optimization\noptimized_prompt = optimizer.run()\n</code></pre>"},{"location":"library/optimizers/protegi/#citation","title":"Citation","text":"<pre><code>@misc{pryzant2023automaticpromptoptimizationgradient,\n    title={Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search}, \n    author={Reid Pryzant and Dan Iter and Jerry Li and Yin Tat Lee and Chenguang Zhu and Michael Zeng},\n    year={2023},\n    eprint={2305.03495},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2305.03495}, \n}\n</code></pre>"},{"location":"library/optimizers/protegi/#source","title":"Source","text":""},{"location":"library/optimizers/protegi/#src.prompt_optimizer.optimizers.protegi.ProtegiOptimizer","title":"<code>ProtegiOptimizer</code>","text":"<p>               Bases: <code>BaseOptimizer</code></p> <p>ProTeGi Optimizer.</p> <p>Based on ProTeGi with Successive Rejects.</p> <pre><code>@misc{pryzant2023automaticpromptoptimizationgradient,\n    title={Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search},\n    author={Reid Pryzant and Dan Iter and Jerry Li and Yin Tat Lee and Chenguang Zhu and Michael Zeng},\n    year={2023},\n    eprint={2305.03495},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2305.03495},\n}\n</code></pre> Source code in <code>src/prompt_optimizer/optimizers/protegi.py</code> <pre><code>class ProtegiOptimizer(BaseOptimizer):\n    \"\"\"\n    ProTeGi Optimizer.\n\n    Based on ProTeGi with Successive Rejects.\n\n    ```\n    @misc{pryzant2023automaticpromptoptimizationgradient,\n        title={Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search},\n        author={Reid Pryzant and Dan Iter and Jerry Li and Yin Tat Lee and Chenguang Zhu and Michael Zeng},\n        year={2023},\n        eprint={2305.03495},\n        archivePrefix={arXiv},\n        primaryClass={cs.CL},\n        url={https://arxiv.org/abs/2305.03495},\n    }\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        client: ClientType,\n        seed_prompts: list[BasePrompt],\n        validation_set: ValidationSetType,\n        max_depth: int,\n        evaluator: Callable[[BasePrompt, ValidationSetType], ScoreType],\n        output_path: Optional[Union[str, Path]] = None,\n        num_feedbacks: int = 3,\n        steps_per_gradient: int = 3,\n        num_resample: int = 3,\n        search_mode: Literal[\"greedy\", \"beam\"] = \"beam\",\n        score_threshold: Optional[Union[float, int]] = None,\n    ):\n        \"\"\"\n        Initialize the ProTeGi optimizer.\n\n        Args:\n            client (ClientType):\n                Language model client to use for prompt generation and feedback.\n            seed_prompts (list[BasePrompt]):\n                List of prompts to seed generation.\n            validation_set (ValidationSetType):\n                Set of examples to evaluate the prompt on.\n            max_depth (int):\n                Maximum iteration depth for prompt generation.\n            evaluator (Callable[[BasePrompt, ValidationSetType], ScoreType]):\n                Function that takes a prompt and the validation data and returns a score.\n            output_path (Union[str, Path], optional):\n                Path to store run results. Should be a .jsonl file path.\n                If None, no outputs will be written to disk. Defaults to None.\n            num_feedbacks (int, optional):\n                Number of feedbacks to generate per prompt. Defaults to 3.\n            steps_per_gradient (int, optional):\n                Number of new prompts to generate per feedback. Defaults to 3.\n            num_resample (int, optional):\n                Number of Monte Carlo rewrites per new prompt generated from feedback. The paper recommends\n                setting this equal to steps_per_gradient. Defaults to 3.\n            search_mode (Literal[\"greedy\", \"beam\"], optional):\n                Mode for filtering prompt candidates after each step. \"greedy\" keeps all prompts from the previous step.\n                \"beam\" keeps only the highest scoring prompt from the previous step. Defaults to \"beam\".\n            score_threshold (float, optional):\n                Threshold for early convergence. If a prompt exceeds this score after any iteration, the optimization loop\n                immediately ends. If set to None, the optimization loop will not terminate early. Defaults to None.\n\n        \"\"\"\n        super().__init__(\n            client=client,\n            seed_prompts=seed_prompts,\n            validation_set=validation_set,\n            max_depth=max_depth,\n            evaluator=evaluator,\n            output_path=output_path,\n        )\n        self.num_feedbacks = num_feedbacks\n        self.steps_per_gradient = steps_per_gradient\n        self.num_resample = num_resample\n        self.search_mode = search_mode\n        self.score_threshold = score_threshold\n\n    def _extract_responses(self, content: str) -&gt; list[str]:\n        \"\"\"\n        Extract the responses between &lt;START&gt; and &lt;END&gt;.\n\n        Args:\n            content (str): Output string from an LLM generation request.\n\n        Returns:\n            list[str]: List of all responses within &lt;START&gt; and &lt;/?END&gt; or &lt;START&gt; and &lt;/?START&gt;.\n\n        \"\"\"\n        pattern = r\"&lt;START&gt;(.*?)(?:&lt;\\/?END&gt;|&lt;\\/?START&gt;)\"\n        matches = re.findall(pattern, content, flags=re.DOTALL)\n        return matches\n\n    def _generate(self, metaprompt_template: str, template_kwargs: dict) -&gt; str:\n        \"\"\"\n        Generate a completion for a given template and kwargs and parse the results.\n\n        Args:\n            metaprompt_template (str): Template for the metaprompt.\n            template_kwargs (dict): Key word arguments to fill the template values.\n            kwargs: Additional kwargs to pass to the OpenAI client.completions.create (e.g. temperature)\n\n        Returns:\n            list[str]: The parsed generation results.\n\n        \"\"\"\n        metaprompt = metaprompt_template.format(**template_kwargs)\n        input = [{\"role\": \"user\", \"content\": metaprompt}]\n        raw_response = self.client.invoke(input=input)\n        response = raw_response.content.strip()\n        return response\n\n    def generate_prompt_candidates(self, *, prompts: list[BasePrompt], **kwargs) -&gt; list[BasePrompt]:\n        \"\"\"Generate prompt candidates using gradients.\"\"\"\n        prompt_candidates = []\n        for prompt in track(prompts, description=\"Generating prompt candidates\", transient=True):\n            # Generate gradients\n            template_kwargs = {\n                \"prompt\": prompt.content,\n                \"error_string\": prompt.metadata.get(\"error_string\", \"\"),\n                \"num_feedbacks\": self.num_feedbacks,\n                \"steps_per_gradient\": self.steps_per_gradient,\n            }\n            raw_gradients = self._generate(metaprompt_template=GENERATE_GRADIENT_PROMPT_TEMPLATE, template_kwargs=template_kwargs)\n            gradients = self._extract_responses(raw_gradients)\n            gradients = gradients[: self.num_feedbacks]\n            # Generate prompts for each gradient\n            for gradient in gradients:\n                template_kwargs.update({\"gradient\": gradient})\n                raw_new_prompts = self._generate(metaprompt_template=INCORPORTATE_GRADIENT_PROMPT_TEMPLATE, template_kwargs=template_kwargs)\n                new_prompts = self._extract_responses(raw_new_prompts)\n                new_prompts = new_prompts[: self.steps_per_gradient]\n                metadata = {\"_origin_prompt\": prompt.content, \"_gradient\": gradient, \"_resampled\": False}\n                new_prompt_candidates = [BasePrompt(content=new_prompt, metadata=metadata) for new_prompt in new_prompts]\n\n                # Resample new prompts\n                for new_prompt in new_prompts:\n                    varied_prompts = [\n                        self._generate(metaprompt_template=RESAMPLING_PROMPT_TEMPLATE, template_kwargs={\"prompt\": new_prompt})\n                        for _ in range(self.num_resample)\n                    ]\n                    metadata = {\"_origin_prompt\": new_prompt, \"_gradient\": None, \"_resampled\": True}\n                    varied_prompts = [BasePrompt(content=new_prompt) for new_prompt in varied_prompts]\n                    new_prompt_candidates.extend(varied_prompts)\n\n                # Save prompts to prompt candidates\n                prompt_candidates.extend(new_prompt_candidates)\n\n        # Add back the initial prompts to the pool\n        prompt_candidates = prompts + prompt_candidates\n\n        return prompt_candidates\n\n    def _get_best_prompt(self, prompts: list[BasePrompt]):\n        \"\"\"Get the highest scoring prompt.\"\"\"\n        if any(prompt.score is None for prompt in prompts):\n            raise ValueError(\"All prompts must be scored before calling this function.\")\n        return max(prompts, key=lambda x: x.score)\n\n    def select_prompt_candidates(self, *, prompts: list[BasePrompt], validation_set: ValidationSetType) -&gt; list[BasePrompt]:\n        \"\"\"Select prompt candidates according to the search mode.\"\"\"\n        self._score_prompts(prompts=prompts, validation_set=validation_set)\n        if self.search_mode == \"greedy\":\n            return prompts\n        elif self.search_mode == \"beam\":\n            return [self._get_best_prompt(prompts=prompts)]\n\n    def check_early_convergence(self, *, all_prompts: list[list[BasePrompt]]):\n        \"\"\"Check if the early convergence criteria is met.\"\"\"\n        if self.score_threshold is None:\n            return False\n\n        # Flatten all iterations\n        prompts = sum(all_prompts, start=[])\n\n        # Check if early convergence criteria is met\n        highest_score = max(prompts, key=lambda x: x.score).score\n        if highest_score &gt;= self.score_threshold:\n            return True\n        return False\n\n    def select_best_prompt(self, *, all_prompts: list[list[BasePrompt]]) -&gt; BasePrompt:\n        \"\"\"Select the top scoring prompt.\"\"\"\n        # Flatten all iterations\n        prompts = sum(all_prompts, start=[])\n\n        # Select the single prompt with the highest score\n        best_prompt = self._get_best_prompt(prompts=prompts)\n        logger.info(f\"Best score: {best_prompt.score:.3f}\")\n        return best_prompt\n</code></pre>"},{"location":"library/optimizers/protegi/#src.prompt_optimizer.optimizers.protegi.ProtegiOptimizer.__init__","title":"<code>__init__(*, client, seed_prompts, validation_set, max_depth, evaluator, output_path=None, num_feedbacks=3, steps_per_gradient=3, num_resample=3, search_mode='beam', score_threshold=None)</code>","text":"<p>Initialize the ProTeGi optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>ClientType</code> <p>Language model client to use for prompt generation and feedback.</p> required <code>seed_prompts</code> <code>list[BasePrompt]</code> <p>List of prompts to seed generation.</p> required <code>validation_set</code> <code>ValidationSetType</code> <p>Set of examples to evaluate the prompt on.</p> required <code>max_depth</code> <code>int</code> <p>Maximum iteration depth for prompt generation.</p> required <code>evaluator</code> <code>Callable[[BasePrompt, ValidationSetType], ScoreType]</code> <p>Function that takes a prompt and the validation data and returns a score.</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to store run results. Should be a .jsonl file path. If None, no outputs will be written to disk. Defaults to None.</p> <code>None</code> <code>num_feedbacks</code> <code>int</code> <p>Number of feedbacks to generate per prompt. Defaults to 3.</p> <code>3</code> <code>steps_per_gradient</code> <code>int</code> <p>Number of new prompts to generate per feedback. Defaults to 3.</p> <code>3</code> <code>num_resample</code> <code>int</code> <p>Number of Monte Carlo rewrites per new prompt generated from feedback. The paper recommends setting this equal to steps_per_gradient. Defaults to 3.</p> <code>3</code> <code>search_mode</code> <code>Literal['greedy', 'beam']</code> <p>Mode for filtering prompt candidates after each step. \"greedy\" keeps all prompts from the previous step. \"beam\" keeps only the highest scoring prompt from the previous step. Defaults to \"beam\".</p> <code>'beam'</code> <code>score_threshold</code> <code>float</code> <p>Threshold for early convergence. If a prompt exceeds this score after any iteration, the optimization loop immediately ends. If set to None, the optimization loop will not terminate early. Defaults to None.</p> <code>None</code> Source code in <code>src/prompt_optimizer/optimizers/protegi.py</code> <pre><code>def __init__(\n    self,\n    *,\n    client: ClientType,\n    seed_prompts: list[BasePrompt],\n    validation_set: ValidationSetType,\n    max_depth: int,\n    evaluator: Callable[[BasePrompt, ValidationSetType], ScoreType],\n    output_path: Optional[Union[str, Path]] = None,\n    num_feedbacks: int = 3,\n    steps_per_gradient: int = 3,\n    num_resample: int = 3,\n    search_mode: Literal[\"greedy\", \"beam\"] = \"beam\",\n    score_threshold: Optional[Union[float, int]] = None,\n):\n    \"\"\"\n    Initialize the ProTeGi optimizer.\n\n    Args:\n        client (ClientType):\n            Language model client to use for prompt generation and feedback.\n        seed_prompts (list[BasePrompt]):\n            List of prompts to seed generation.\n        validation_set (ValidationSetType):\n            Set of examples to evaluate the prompt on.\n        max_depth (int):\n            Maximum iteration depth for prompt generation.\n        evaluator (Callable[[BasePrompt, ValidationSetType], ScoreType]):\n            Function that takes a prompt and the validation data and returns a score.\n        output_path (Union[str, Path], optional):\n            Path to store run results. Should be a .jsonl file path.\n            If None, no outputs will be written to disk. Defaults to None.\n        num_feedbacks (int, optional):\n            Number of feedbacks to generate per prompt. Defaults to 3.\n        steps_per_gradient (int, optional):\n            Number of new prompts to generate per feedback. Defaults to 3.\n        num_resample (int, optional):\n            Number of Monte Carlo rewrites per new prompt generated from feedback. The paper recommends\n            setting this equal to steps_per_gradient. Defaults to 3.\n        search_mode (Literal[\"greedy\", \"beam\"], optional):\n            Mode for filtering prompt candidates after each step. \"greedy\" keeps all prompts from the previous step.\n            \"beam\" keeps only the highest scoring prompt from the previous step. Defaults to \"beam\".\n        score_threshold (float, optional):\n            Threshold for early convergence. If a prompt exceeds this score after any iteration, the optimization loop\n            immediately ends. If set to None, the optimization loop will not terminate early. Defaults to None.\n\n    \"\"\"\n    super().__init__(\n        client=client,\n        seed_prompts=seed_prompts,\n        validation_set=validation_set,\n        max_depth=max_depth,\n        evaluator=evaluator,\n        output_path=output_path,\n    )\n    self.num_feedbacks = num_feedbacks\n    self.steps_per_gradient = steps_per_gradient\n    self.num_resample = num_resample\n    self.search_mode = search_mode\n    self.score_threshold = score_threshold\n</code></pre>"},{"location":"library/optimizers/protegi/#src.prompt_optimizer.optimizers.protegi.ProtegiOptimizer.check_early_convergence","title":"<code>check_early_convergence(*, all_prompts)</code>","text":"<p>Check if the early convergence criteria is met.</p> Source code in <code>src/prompt_optimizer/optimizers/protegi.py</code> <pre><code>def check_early_convergence(self, *, all_prompts: list[list[BasePrompt]]):\n    \"\"\"Check if the early convergence criteria is met.\"\"\"\n    if self.score_threshold is None:\n        return False\n\n    # Flatten all iterations\n    prompts = sum(all_prompts, start=[])\n\n    # Check if early convergence criteria is met\n    highest_score = max(prompts, key=lambda x: x.score).score\n    if highest_score &gt;= self.score_threshold:\n        return True\n    return False\n</code></pre>"},{"location":"library/optimizers/protegi/#src.prompt_optimizer.optimizers.protegi.ProtegiOptimizer.generate_prompt_candidates","title":"<code>generate_prompt_candidates(*, prompts, **kwargs)</code>","text":"<p>Generate prompt candidates using gradients.</p> Source code in <code>src/prompt_optimizer/optimizers/protegi.py</code> <pre><code>def generate_prompt_candidates(self, *, prompts: list[BasePrompt], **kwargs) -&gt; list[BasePrompt]:\n    \"\"\"Generate prompt candidates using gradients.\"\"\"\n    prompt_candidates = []\n    for prompt in track(prompts, description=\"Generating prompt candidates\", transient=True):\n        # Generate gradients\n        template_kwargs = {\n            \"prompt\": prompt.content,\n            \"error_string\": prompt.metadata.get(\"error_string\", \"\"),\n            \"num_feedbacks\": self.num_feedbacks,\n            \"steps_per_gradient\": self.steps_per_gradient,\n        }\n        raw_gradients = self._generate(metaprompt_template=GENERATE_GRADIENT_PROMPT_TEMPLATE, template_kwargs=template_kwargs)\n        gradients = self._extract_responses(raw_gradients)\n        gradients = gradients[: self.num_feedbacks]\n        # Generate prompts for each gradient\n        for gradient in gradients:\n            template_kwargs.update({\"gradient\": gradient})\n            raw_new_prompts = self._generate(metaprompt_template=INCORPORTATE_GRADIENT_PROMPT_TEMPLATE, template_kwargs=template_kwargs)\n            new_prompts = self._extract_responses(raw_new_prompts)\n            new_prompts = new_prompts[: self.steps_per_gradient]\n            metadata = {\"_origin_prompt\": prompt.content, \"_gradient\": gradient, \"_resampled\": False}\n            new_prompt_candidates = [BasePrompt(content=new_prompt, metadata=metadata) for new_prompt in new_prompts]\n\n            # Resample new prompts\n            for new_prompt in new_prompts:\n                varied_prompts = [\n                    self._generate(metaprompt_template=RESAMPLING_PROMPT_TEMPLATE, template_kwargs={\"prompt\": new_prompt})\n                    for _ in range(self.num_resample)\n                ]\n                metadata = {\"_origin_prompt\": new_prompt, \"_gradient\": None, \"_resampled\": True}\n                varied_prompts = [BasePrompt(content=new_prompt) for new_prompt in varied_prompts]\n                new_prompt_candidates.extend(varied_prompts)\n\n            # Save prompts to prompt candidates\n            prompt_candidates.extend(new_prompt_candidates)\n\n    # Add back the initial prompts to the pool\n    prompt_candidates = prompts + prompt_candidates\n\n    return prompt_candidates\n</code></pre>"},{"location":"library/optimizers/protegi/#src.prompt_optimizer.optimizers.protegi.ProtegiOptimizer.get_all_prompts","title":"<code>get_all_prompts(include_candidates=False)</code>","text":"<p>Get all the prompts from the latest training run.</p> <p>The default behavior returns a list of lists, where each internal list contains the retained candidates after one iteration step. Setting include_candidates to True will also include all generated candidate prompts.</p> <p>Parameters:</p> Name Type Description Default <code>include_candidates</code> <code>bool</code> <p>Whether to include all the candidate prompts in the output. If True, candidate prompts from each iteration will be included. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[list[BasePrompt]]</code> <p>list[list[BasePrompt]]: List of lists where each list contains the prompts from each iteration. E.g. list[0] contains prompts from the first iteration, list[1] the second, etc. If include_candidates is False, each inner list contains only the retained prompts at each iteration. If include_candidates is True, each inner list contains all candidate prompts at each iteration, including those that were discarded.</p> Source code in <code>src/prompt_optimizer/optimizers/base.py</code> <pre><code>def get_all_prompts(self, include_candidates: bool = False) -&gt; list[list[BasePrompt]]:\n    \"\"\"\n    Get all the prompts from the latest training run.\n\n    The default behavior returns a list of lists, where each internal list contains the\n    retained candidates after one iteration step.\n    Setting include_candidates to True will also include all generated candidate prompts.\n\n    Args:\n        include_candidates (bool, optional):\n            Whether to include all the candidate prompts in the output.\n            If True, candidate prompts from each iteration will be included.\n            Defaults to False.\n\n    Returns:\n        list[list[BasePrompt]]:\n            List of lists where each list contains the prompts from each iteration.\n            E.g. list[0] contains prompts from the first iteration, list[1] the second, etc.\n            If include_candidates is False, each inner list contains only the retained prompts at each iteration.\n            If include_candidates is True, each inner list contains all candidate prompts at each iteration,\n            including those that were discarded.\n\n    \"\"\"\n    # Decide whether to include candidates\n    if include_candidates:\n        all_prompts = self._g\n    else:\n        all_prompts = self._p\n\n    return all_prompts\n</code></pre>"},{"location":"library/optimizers/protegi/#src.prompt_optimizer.optimizers.protegi.ProtegiOptimizer.run","title":"<code>run()</code>","text":"<p>Run the optimization pipeline.</p> Source code in <code>src/prompt_optimizer/optimizers/base.py</code> <pre><code>def run(self) -&gt; BasePrompt:\n    \"\"\"Run the optimization pipeline.\"\"\"\n    # Initialize objects\n    self._p = [self.seed_prompts]\n    self._g = [self.seed_prompts]\n    # Iterate until max depth\n    for t in track(range(1, self.max_depth + 1), description=\"Step\", total=self.max_depth):\n        # Generate prompt candidates\n        g_t = self.generate_prompt_candidates(prompts=self._p[t - 1], validation_set=self.validation_set)\n        self._g.append(g_t)\n        # Select prompt candidates\n        p_t = self.select_prompt_candidates(prompts=self._g[t], validation_set=self.validation_set)\n        self._p.append(p_t)\n        # Check for early convergence\n        if self.check_early_convergence(all_prompts=self._p):\n            break\n\n    # Save prompts if requested\n    self.save_prompts(output_path=self.output_path)\n\n    # Return best prompt\n    return self.select_best_prompt(all_prompts=self._p)\n</code></pre>"},{"location":"library/optimizers/protegi/#src.prompt_optimizer.optimizers.protegi.ProtegiOptimizer.save_prompts","title":"<code>save_prompts(output_path)</code>","text":"<p>Save prompts in jsonl format.</p> Source code in <code>src/prompt_optimizer/optimizers/base.py</code> <pre><code>def save_prompts(self, output_path: Optional[Union[str, Path]]):\n    \"\"\"Save prompts in jsonl format.\"\"\"\n    # Exit if no output path is set\n    if self.output_path is None:\n        return\n\n    # Get and deduplicate prompts\n    prompts = sum(self._p, start=[])\n    prompts = list(set(prompts))\n\n    # Save the prompts to the file\n    lines = [prompt.model_dump_json() for prompt in prompts]\n    with open(output_path, \"w\") as f:\n        for line in lines:\n            f.write(line)\n            f.write(\"\\n\")\n</code></pre>"},{"location":"library/optimizers/protegi/#src.prompt_optimizer.optimizers.protegi.ProtegiOptimizer.select_best_prompt","title":"<code>select_best_prompt(*, all_prompts)</code>","text":"<p>Select the top scoring prompt.</p> Source code in <code>src/prompt_optimizer/optimizers/protegi.py</code> <pre><code>def select_best_prompt(self, *, all_prompts: list[list[BasePrompt]]) -&gt; BasePrompt:\n    \"\"\"Select the top scoring prompt.\"\"\"\n    # Flatten all iterations\n    prompts = sum(all_prompts, start=[])\n\n    # Select the single prompt with the highest score\n    best_prompt = self._get_best_prompt(prompts=prompts)\n    logger.info(f\"Best score: {best_prompt.score:.3f}\")\n    return best_prompt\n</code></pre>"},{"location":"library/optimizers/protegi/#src.prompt_optimizer.optimizers.protegi.ProtegiOptimizer.select_prompt_candidates","title":"<code>select_prompt_candidates(*, prompts, validation_set)</code>","text":"<p>Select prompt candidates according to the search mode.</p> Source code in <code>src/prompt_optimizer/optimizers/protegi.py</code> <pre><code>def select_prompt_candidates(self, *, prompts: list[BasePrompt], validation_set: ValidationSetType) -&gt; list[BasePrompt]:\n    \"\"\"Select prompt candidates according to the search mode.\"\"\"\n    self._score_prompts(prompts=prompts, validation_set=validation_set)\n    if self.search_mode == \"greedy\":\n        return prompts\n    elif self.search_mode == \"beam\":\n        return [self._get_best_prompt(prompts=prompts)]\n</code></pre>"}]}